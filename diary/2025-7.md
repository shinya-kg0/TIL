# 7月の目標


## 7/1

### 振り返り
- 機能仕様について、追加された関数について言及した方がいいかを確認 → 必要なし
- 機能仕様の更新

- Fluent Pythonを読み進める
  - `@property`デコレータはその関数における特殊メソッドをオーバーライドして、関数の呼び出し方によって返す関数を変更する
  - ディスクリプタについて軽く理解

```py
# 具体例

class Person:
    def __init__(self, name):
        self._name = name

    @property # getter
    def name(self):
        return self._name

    @name.setter # setter
    def name(self, value):
        if not value:
            raise ValueError("名前は空にできません")
        self._name = value

    @name.deleter # deleter
    def name(self):
        print("名前の削除")
        del self._name

taro = Person("太郎") # 値の取得を行う。getterが呼び出される
print(taro.name) 
# 太郎
taro.name = "山田太郎" # 値の設定を行う。setterが呼び出される
print(taro.name)
# 山田太郎
del taro.name # 値の削除を行う。deleterが呼び出される
# 名前の削除
print(taro.name) 
# AttributeError: 'Person' object has no attribute '_name'
```

- RAG構築の本を進める
  - リソースの設定ができた
  - 実際にリクエストを送ってみたが、リソースがないと出ている。
  - タイポはないはずだがよくわからない
  - .pyからリクエスト送ったら返ってきた
    - zshの問題か、curlの書き方の問題？？

## 7/2

### 振り返り
- 納品物の確認
  - Wikiのクローンがうまくいかなかった
  - 新しいリポジトリでブランチを明示的に設定したら解決した
  - `git clone --branch master ssh://git@hogehoge.com:2222/xxx/yyy/zzz.wiki.git`

- Fluent Pythonを読み進める
  - いったん一通りさらえたので、復習目的ではじめから斜め読みしていく
  - リソースは他に使っていく

- RAG構築の本を進める
  - 本実装の検索部分のコンポーネントはAzure AI Searchを使う
  - 複数のドキュメントが格納されているインデックスを使って情報を管理する
  - それぞれのドキュメントは特定のスキーマに基づいて構成されている
  - 例えば、主キー、content, category, rating
  - データの持ち方
    - レプリカ：インデックスの複製をいくつか持っておくことで同時に多くの検索リクエストを処理できる（負荷分散、高可用性）
    - パーティション：インデックスを複数の部分に分割する仕組み。複数サーバーで並行して処理できる

## 7/3

### 振り返り
- 次回追加機能のロジック確認

- Fluent Python復習

- RAG構築コーディング

- Reactチュートリアル進める

## 7/4

### 振り返り
- MRの作成＋レビュー依頼
  - リモートの変更を取り込みrebaseしpush完了
  - flake8は`--ignore=E501`をつけてチェックする。（デフォルトのリストを上書きするので挙動が少し変わる）

- FluentPython復習

- RAG構築続き
  - 実際にリソースを立てて、動かしてみる

- Reactチュートリアル進める

## 7/7

### 振り返り
- RAG構築続き
  - なんとか接続完了した
    - APIのバージョンは、モデルのバージョンと異なる
  - RAGではアプリ開発と評価方法が少し違う
    - ユーザーからの質問に対してどれだけ正確で信頼性のある回答を返せるかが大事
    - 人手による評価、LLMによる評価がある
    - Prompt FlowというRAGの評価ツールがある
  - RAGの評価指標
    - 「ユーザーの質問」「コンテキスト」「回答」「Ground Truth」が大事
    - 回答とGround Truthの比較
      - 差異がある場合は、RAGの一連の流れ、そもそもユーザーの質問が適切でない、Retrieverが適切に情報を取得できていない、Generatorが正確な回答が生成できていない可能性がある
    - コンテキストとユーザーの質問の整合性
      - 適切に情報を検索できているか？Retrieverの調整が必要
    - 回答が、コンテキストに基づいて生成されたか
      - Generatorが適切な回答を生成するように調整が必要
  - Prompt Flowによる評価指標
    - 根拠性：「回答」「コンテキスト」
      - Generatorが生成した回答が、コンテキスト内の事実や現実にどれだけ基づいているかを評価
    - 類似性：「回答」「Graund Truth」
      - LLMを使ってGeneratorが生成した回答がGround Truthにどれだけ類似しているか測定
    - 関連性：「質問」「回答」
      - モデルの回答が、質問に対してどれだけ適切に関連しているか
    - コヒーレンス、流暢性：「回答」
      - 回答全体の文法的な正しさや自然さを評価
    - 実際に出力したcsvファイルをPrompt Flowを使って評価した
  - RAGの改善方法
    - セマンティックチャンキング：まずは小さなフレーズ単位で分割し埋め込みを行う、類似度が高ければ結合する。
      - 意味のまとまりがある分のグループがチャンクとして形成される。内容に応じた可変長のチャンクができる
    - HyDE(Hypothetical Document Embedding)：クエリから仮の回答を作ってそれとドキュメントの類似度を比較する
      - クエリとドキュメントを比較すると言い回しや構成が違うので意図しないコンテキストになる可能性。クエリを一旦仮のドキュメントにすることで適正に比較できる！