# 概要

巨大なデータセット（S3などのデータレイク）のための高性能なオープンテーブルフォーマット

## どんな課題を解決する？

従来のデータレイクは「単なるファイルの集まり」

- データの不整合
  - 書き込み中のエラーが起きると中途半端なデータが残る
- パフォーマンス低下
  - 目的のデータを探すために、多くのファイルをスキャンしなければならない
- スキーマ変更の難しさ
  - カラムの追加や、名前の変更が手間

→ これらの課題を解決し、**データレイクをDWHのように扱える**ようにします。

## 主な特徴

- ACIDトランザクション: データの一貫性を保つ
- バージョン管理: 過去の地点の読み出し、データ復旧など
- スキーマ変更: カラムの追加、削除、リネーム
- パーティション: データの分類ルールを物理的な配置を変えず変更可能


## Icebergの構造

| **レイヤー** | **役割** |
| --- | --- |
| **Iceberg Catalog** | 現在の「最新の状態」がどのメタデータファイルを指しているかを管理する場所。 |
| **Metadata Layer** | テーブルの構造（スキーマ）、パーティション、スナップショットの履歴を記録したファイル。 |
| **Data Layer** | 実際のデータ本体（Parquet, Avro, ORC形式など）。 |



| **比較項目** | **S3 + Iceberg (データレイクハウス)** | **Amazon Redshift (DWH)** | **Snowflake (クラウドDWH)** |
| --- | --- | --- | --- |
| **主なコンセプト** | **オープン・柔軟** | **AWSエコシステム・高速** | **マルチクラウド・運用レス** |
| **データの保存先** | 自分のS3バケット (オープン形式) | Redshift専用ストレージ | Snowflake管理のストレージ |
| **データの形式** | **Apache Iceberg (Parquet等)** | 独自フォーマット (非公開) | 独自フォーマット (非公開) |
| **得意なツール** | Spark, Trino, Flink, Athenaなど自由 | Redshift SQL, Spectrum | Snowflake SQL |
| **運用負荷** | 中（カタログ管理やコンパクションが必要） | 中〜高（ノード管理が必要な場合あり） | **低（フルマネージドでほぼ自動）** |
| **コスト構造** | S3代 + 使った分の計算リソース代 | インスタンス稼働時間（またはサーバーレス） | クレジット消費（計算中のみ課金） |
| **ベンダーロックイン** | **なし（いつでもツールを乗り換え可）** | あり（AWSに最適化） | あり（Snowflakeに最適化） |
| **主な利用シーン** | 大規模な機械学習、多様なエンジンの併用 | AWS中心の既存BI・分析、高速集計 | 全社的なデータ基盤、運用の効率化 |

1. 「S3 + Iceberg」が向いている人
    - データの量が膨大（ペタバイト級）で、ストレージコストを極限まで抑えたい。
    - SQLだけでなく、**PythonやSparkを使って機械学習（AI）**もゴリゴリやりたい。
    - 特定のベンダーに縛られず、常に最新の技術（TrinoやDuckDBなど）を試したい。

2. 「Amazon Redshift」が向いている人
    - すでにAWSをフル活用しており、AWS GlueやS3との親和性を最優先したい。
    - 決まった形式のレポートやBIツール向けの高速な集計基盤が欲しい。
    - AWSの予約インスタンス（RI）などでコストを固定化・最適化したい。

3. 「Snowflake」が向いている人
    - 「運用に時間をかけたくない」。チューニングやメンテナンスを自動化したい。
    - 社外とのデータ共有（データシェアリング）を安全かつ簡単に行いたい。
    - マルチクラウド（AWS, Azure, GCP）をまたいでデータを統合したい。